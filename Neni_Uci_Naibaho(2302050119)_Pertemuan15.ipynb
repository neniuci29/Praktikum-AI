{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO/zYMMKiVxCJXTwknl62bu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neniuci29/Praktikum-AI/blob/main/Neni_Uci_Naibaho(2302050119)_Pertemuan15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCInuL6D3lY3"
      },
      "outputs": [],
      "source": [
        "# Library yang sering digunakan\n",
        "import os, shutil\n",
        "import zipfile\n",
        "import random\n",
        "from random import sample\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm as tq\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries untuk pemrosesan data gambar\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import skimage\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "from skimage.transform import rotate, AffineTransform, warp\n",
        "from skimage import img_as_ubyte\n",
        "from skimage.exposure import adjust_gamma\n",
        "from skimage.util import random_noise"
      ],
      "metadata": {
        "id": "loPrDpWf34aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries untuk pembangunan model\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, layers\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.layers import InputLayer, Conv2D, SeparableConv2D, MaxPooling2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "l-t7IVm64DuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "metadata": {
        "id": "rudMPWm14RyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mencetak versi TensorFlow yang sedang digunakan\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "OR9YEq8O4imN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import module yang disediakan google colab untuk kebutuhan upload file\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "YeDu1AD-6TkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download kaggle dataset and unzip the file\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d tolgadincer/labeled-chest-xray-images\n",
        "!unzip -q labeled-chest-xray-images.zip"
      ],
      "metadata": {
        "id": "IDS3tXBd516H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wkp8bEpr7HNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil"
      ],
      "metadata": {
        "id": "3mztKr0O61H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Direktori awal untuk train dan test\n",
        "train_dir = \"chest_xray/train\"\n",
        "test_dir = \"chest_xray/test\"\n",
        "\n",
        "# Direktori baru untuk dataset gabungan\n",
        "combined_dir = \"chest_xray/dataset\""
      ],
      "metadata": {
        "id": "2jx2_Wc16-el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat direktori baru untuk dataset gabungan\n",
        "os.makedirs(combined_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "UAir0h9j7GsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salin file dan folder dari train\n",
        "for category in os.listdir(train_dir):\n",
        "    category_dir = os.path.join(train_dir, category)\n",
        "    if os.path.isdir(category_dir):\n",
        "        shutil.copytree(category_dir, os.path.join(combined_dir, category), dirs_exist_ok=True)\n",
        "\n",
        "# Salin file dan folder dari test\n",
        "for category in os.listdir(test_dir):\n",
        "    category_dir = os.path.join(test_dir, category)\n",
        "    if os.path.isdir(category_dir):\n",
        "        shutil.copytree(category_dir, os.path.join(combined_dir, category), dirs_exist_ok=True)"
      ],
      "metadata": {
        "id": "z3IKV_d67QMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat kamus yang menyimpan gambar untuk setiap kelas dalam data\n",
        "lung_image = {}\n",
        "\n",
        "# Tentukan path sumber train\n",
        "path = \"chest_xray/\"\n",
        "path_sub = os.path.join(path, \"dataset\")\n",
        "for i in os.listdir(path_sub):\n",
        "    lung_image[i] = os.listdir(os.path.join(path_sub, i))\n",
        "\n",
        "# Menampilkan secara acak 5 gambar di bawah setiap dari 2 kelas dari data.\n",
        "# Anda akan melihat gambar yang berbeda setiap kali kode ini dijalankan.\n",
        "path_sub = \"chest_xray/dataset/\"\n",
        "\n",
        "# Menampilkan secara acak 5 gambar di bawah setiap kelas dari data latih\n",
        "fig, axs = plt.subplots(len(lung_image.keys()), 5, figsize=(15, 15))\n",
        "\n",
        "for i, class_name in enumerate(os.listdir(path_sub)):\n",
        "    images = np.random.choice(lung_image[class_name], 5, replace=False)\n",
        "\n",
        "    for j, image_name in enumerate(images):\n",
        "        img_path = os.path.join(path_sub, class_name, image_name)\n",
        "        img = Image.open(img_path).convert(\"L\")  # Konversi menjadi skala keabuan\n",
        "        axs[i, j].imshow(img, cmap='gray')\n",
        "        axs[i, j].set(xlabel=class_name, xticks=[], yticks=[])\n",
        "\n",
        "\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "08Xgb4To7aw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisikan path sumber\n",
        "lung_path = \"chest_xray/dataset/\"\n",
        "\n",
        "# Buat daftar yang menyimpan data untuk setiap nama file, path file, dan label dalam data\n",
        "file_name = []\n",
        "labels = []\n",
        "full_path = []\n",
        "\n",
        "# Dapatkan nama file gambar, path file, dan label satu per satu dengan looping, dan simpan sebagai dataframe\n",
        "for path, subdirs, files in os.walk(lung_path):\n",
        "    for name in files:\n",
        "        full_path.append(os.path.join(path, name))\n",
        "        labels.append(path.split('/')[-1])\n",
        "        file_name.append(name)\n",
        "\n",
        "distribution_train = pd.DataFrame({\"path\":full_path, 'file_name':file_name, \"labels\":labels})\n",
        "\n",
        "# Plot distribusi gambar di setiap kelas\n",
        "Label = distribution_train['labels']\n",
        "plt.figure(figsize = (6,6))\n",
        "sns.set_style(\"darkgrid\")\n",
        "plot_data = sns.countplot(Label)"
      ],
      "metadata": {
        "id": "C-rRqdIM7wIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat fungsi untuk melakukan rotasi berlawanan arah jarum jam\n",
        "def anticlockwise_rotation(img):\n",
        "    img = cv2.cvtColor(img, 0)\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    sudut = random.randint(0,180)\n",
        "    return rotate(img, sudut)\n",
        "\n",
        "# Membuat fungsi untuk melakukan rotasi searah jarum jam\n",
        "def clockwise_rotation(img):\n",
        "    img = cv2.cvtColor(img, 0)\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    sudut = random.randint(0,180)\n",
        "    return rotate(img, -sudut)\n",
        "\n",
        "# Membuat fungsi untuk membalik gambar secara vertikal dari atas ke bawah\n",
        "def flip_up_down(img):\n",
        "    img = cv2.cvtColor(img, 0)\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    return np.flipud(img)\n",
        "\n",
        "# Membuat fungsi untuk memberikan efek peningkatan kecerahan pada gambar\n",
        "def add_brightness(img):\n",
        "    img = cv2.cvtColor(img, 0)\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    img = adjust_gamma(img, gamma=0.5,gain=1)\n",
        "    return img\n",
        "\n",
        "# Membuat fungsi untuk memberikan efek blur pada gambar\n",
        "def blur_image(img):\n",
        "    img = cv2.cvtColor(img, 0)\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    return cv2.GaussianBlur(img, (9,9),0)\n",
        "\n",
        "# Membuat fungsi untuk memberikan efek pergeseran acak pada gambar\n",
        "def sheared(img):\n",
        "    img = cv2.cvtColor(img, 0)\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    transform = AffineTransform(shear=0.2)\n",
        "    shear_image = warp(img, transform, mode=\"wrap\")\n",
        "    return shear_image\n",
        "\n",
        "# Membuat fungsi untuk melakukan pergeseran melengkung pada gambar\n",
        "def warp_shift(img):\n",
        "    img = cv2.cvtColor(img, 0)\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    transform = AffineTransform(translation=(0,40))\n",
        "    warp_image = warp(img, transform, mode=\"wrap\")\n",
        "    return warp_image"
      ],
      "metadata": {
        "id": "hpKx9V-G8AjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat variabel transformasi yang akan menyimpan semua proses pra-pemrosesan yang telah dilakukan sebelumnya\n",
        "transformations = { 'rotate anticlockwise': anticlockwise_rotation,\n",
        "                    'rotate clockwise': clockwise_rotation,\n",
        "                    'warp shift': warp_shift,\n",
        "                    'blurring image': blur_image,\n",
        "                    'add brightness' : add_brightness,\n",
        "                    'flip up down': flip_up_down,\n",
        "                    'shear image': sheared\n",
        "                  }\n",
        "\n",
        "images_path=\"chest_xray/dataset/NORMAL\" # Path untuk gambar asli\n",
        "augmented_path=\"chest_xray/dataset/NORMAL\" # Path untuk gambar yang sudah diaugmentasi\n",
        "images=[] # Penyimpanan gambar yang telah melalui pra-pemrosesan\n",
        "\n",
        "# Baca nama gambar dari folder dan tambahkan path ke dalam array \"images\"\n",
        "for im in os.listdir(images_path):\n",
        "    images.append(os.path.join(images_path,im))\n",
        "\n",
        "# Jumlah gambar yang akan ditambahkan dengan hasil transformasi augmentasi, jumlahnya disesuaikan sesuai kebutuhan\n",
        "# Variabel untuk melakukan iterasi sampai jumlah gambar yang ditentukan dalam images_to_generate\n",
        "images_to_generate=2000\n",
        "i=1\n",
        "\n",
        "while i<=images_to_generate:\n",
        "    image=random.choice(images)\n",
        "    try:\n",
        "        original_image = io.imread(image)\n",
        "        transformed_image=None\n",
        "        n = 0      # Variabel untuk melakukan iterasi sampai jumlah transformasi yang akan diterapkan\n",
        "        transformation_count = random.randint(1, len(transformations)) # Pilih jumlah transformasi acak yang akan diterapkan pada gambar\n",
        "\n",
        "        while n <= transformation_count:\n",
        "            key = random.choice(list(transformations)) # Secara acak memilih dan memanggil metode\n",
        "            transformed_image = transformations[key](original_image)\n",
        "            n = n + 1\n",
        "\n",
        "        new_image_path= \"%s/augmented_image_%s.jpg\" %(augmented_path, i)\n",
        "        transformed_image = img_as_ubyte(transformed_image)  # Mengonversi gambar ke format byte yang tidak ditandatangani, dengan nilai dalam [0, 255]\n",
        "        cv2.imwrite(new_image_path, transformed_image)  # Simpan hasil transformasi augmentasi pada gambar ke path yang ditentukan\n",
        "        i =i+1\n",
        "    except ValueError as e:\n",
        "        print('could not read the',image ,':',e,'hence skipping it.')"
      ],
      "metadata": {
        "id": "WbWpKvMc8RgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisikan path sumber\n",
        "lung_path = \"chest_xray/dataset/\"\n",
        "\n",
        "# Buat daftar yang menyimpan data untuk setiap nama file, path file, dan label dalam data\n",
        "file_name = []\n",
        "labels = []\n",
        "full_path = []\n",
        "\n",
        "# Dapatkan nama file gambar, path file, dan label satu per satu dengan looping, dan simpan sebagai dataframe\n",
        "for path, subdirs, files in os.walk(lung_path):\n",
        "    for name in files:\n",
        "        full_path.append(os.path.join(path, name))\n",
        "        labels.append(path.split('/')[-1])\n",
        "        file_name.append(name)\n",
        "\n",
        "distribution_train = pd.DataFrame({\"path\":full_path, 'file_name':file_name, \"labels\":labels})\n",
        "\n",
        "# Plot distribusi gambar di setiap kelas\n",
        "Label = distribution_train['labels']\n",
        "plt.figure(figsize = (6,6))\n",
        "sns.set_style(\"darkgrid\")\n",
        "plot_data = sns.countplot(Label)"
      ],
      "metadata": {
        "id": "nO8dnEuf8xV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Panggil variabel mypath yang menampung folder dataset gambar\n",
        "mypath= 'chest_xray/dataset/'\n",
        "\n",
        "file_name = []\n",
        "labels = []\n",
        "full_path = []\n",
        "for path, subdirs, files in os.walk(mypath):\n",
        "    for name in files:\n",
        "        full_path.append(os.path.join(path, name))\n",
        "        labels.append(path.split('/')[-1])\n",
        "        file_name.append(name)\n",
        "\n",
        "# Memasukkan variabel yang sudah dikumpulkan pada looping di atas menjadi sebuah dataframe agar rapi\n",
        "df = pd.DataFrame({\"path\":full_path,'file_name':file_name,\"labels\":labels})\n",
        "# Melihat jumlah data gambar pada masing-masing label\n",
        "df.groupby(['labels']).size()"
      ],
      "metadata": {
        "id": "A3oJIhCG88xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variabel yang digunakan pada pemisahan data ini di mana variabel x = data path dan y = data labels\n",
        "\n",
        "X= df['path']\n",
        "y= df['labels']\n",
        "\n",
        "# Split dataset awal menjadi data train dan test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=300)"
      ],
      "metadata": {
        "id": "xjCzUNOh9ELB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyatukan ke dalam masing-masing dataframe\n",
        "df_tr = pd.DataFrame({'path':X_train,'labels':y_train,'set':'train'})\n",
        "df_te = pd.DataFrame({'path':X_test,'labels':y_test,'set':'test'})"
      ],
      "metadata": {
        "id": "5-4-HN-I9Rf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gabungkan DataFrame df_tr dan df_te\n",
        "df_all = pd.concat([df_tr, df_te], ignore_index=True)\n",
        "\n",
        "print('===================================================== \\n')\n",
        "print(df_all.groupby(['set', 'labels']).size(), '\\n')\n",
        "print('===================================================== \\n')\n",
        "\n",
        "# Cek sampel data\n",
        "print(df_all.sample(5))\n",
        "\n",
        "# Memanggil dataset asli yang berisi keseluruhan data gambar yang sesuai dengan labelnya\n",
        "datasource_path = \"chest_xray/dataset/\"\n",
        "# Membuat variabel Dataset, tempat menampung data yang telah dilakukan pembagian data training dan testing\n",
        "dataset_path = \"Dataset-Final/\""
      ],
      "metadata": {
        "id": "ROi7dXwU9XnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in tq(df_all.iterrows()):\n",
        "    # Deteksi filepath\n",
        "    file_path = row['path']\n",
        "    if os.path.exists(file_path) == False:\n",
        "        # Original logic was row['image'], corrected to use file_path to extract name\n",
        "        file_path = os.path.join(datasource_path,row['labels'], os.path.basename(file_path).split('.')[0])\n",
        "\n",
        "    # Buat direktori tujuan folder\n",
        "    if os.path.exists(os.path.join(dataset_path,row['set'],row['labels'])) == False:\n",
        "        os.makedirs(os.path.join(dataset_path,row['set'],row['labels']))\n",
        "\n",
        "    # Tentukan tujuan file\n",
        "    destination_file_name = os.path.basename(file_path)\n",
        "    file_dest = os.path.join(dataset_path,row['set'],row['labels'],destination_file_name)\n",
        "\n",
        "    # Salin file dari sumber ke tujuan\n",
        "    if os.path.exists(file_dest) == False:\n",
        "        shutil.copy2(file_path,file_dest)"
      ],
      "metadata": {
        "id": "J-aPF2O49iXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisikan direktori training dan test\n",
        "TRAIN_DIR = \"Dataset-Final/train/\"\n",
        "TEST_DIR = \"Dataset-Final/test/\"\n",
        "\n",
        "train_normal = os.path.join(TRAIN_DIR + '/NORMAL')\n",
        "train_pneumonia = os.path.join(TRAIN_DIR + '/PNEUMONIA')\n",
        "test_normal = os.path.join(TEST_DIR + '/NORMAL')\n",
        "test_pneumonia = os.path.join(TEST_DIR + '/PNEUMONIA')\n",
        "\n",
        "print(\"Total number of normal images in training set: \",len(os.listdir(train_normal)))\n",
        "print(\"Total number of pneumonic images in training set: \",len(os.listdir(train_pneumonia)))\n",
        "print(\"Total number of normal images in test set: \",len(os.listdir(test_normal)))\n",
        "print(\"Total number of pneumonic images in test set: \",len(os.listdir(test_pneumonia)))"
      ],
      "metadata": {
        "id": "qCRmgkzk9qNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat objek ImageDataGenerator yang menormalkan gambar\n",
        "datagen = ImageDataGenerator(rescale=1/255.,\n",
        "                             validation_split = 0.2)\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(TRAIN_DIR,\n",
        "                                              batch_size=32,\n",
        "                                              target_size=(150,150),\n",
        "                                              color_mode=\"grayscale\",\n",
        "                                              class_mode='binary',\n",
        "                                              subset='training',\n",
        "                                              shuffle=True)\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(TRAIN_DIR,\n",
        "                                                     batch_size=32,\n",
        "                                                     target_size=(150,150),\n",
        "                                                     color_mode=\"grayscale\",\n",
        "                                                     class_mode='binary',\n",
        "                                                     subset='validation',\n",
        "                                                     shuffle=False)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(TEST_DIR,\n",
        "                                                  batch_size=1,\n",
        "                                                  target_size=(150,150),\n",
        "                                                  color_mode=\"grayscale\",\n",
        "                                                  class_mode='binary',\n",
        "                                                  shuffle=False)"
      ],
      "metadata": {
        "id": "aisBtjHK9yH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.backend.clear_session()\n",
        "\n",
        "####################### Init sequential model ##################################\n",
        "model_1 = Sequential()\n",
        "\n",
        "# ######################### Input layer with Fully Connected Layer ################################\n",
        "# 1st Convolutional layer, Batch Normalization layer, and Pooling layer\n",
        "model_1.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(150,150,1)))\n",
        "model_1.add(BatchNormalization())\n",
        "model_1.add(MaxPool2D((2, 2)))\n",
        "\n",
        "# 2nd Convolutional layer, Batch Normalization layer, and Pooling layer\n",
        "model_1.add(Conv2D(32, (4, 4),padding='same', activation='relu'))\n",
        "model_1.add(BatchNormalization())\n",
        "model_1.add(MaxPool2D((2, 2)))\n",
        "\n",
        "# 3rd Convolutional layer, Batch Normalization layer, and Pooling layer\n",
        "model_1.add(Conv2D(32, (7, 7), padding='same', activation='relu'))\n",
        "model_1.add(BatchNormalization())\n",
        "model_1.add(MaxPool2D((2, 2)))\n",
        "\n",
        "# Flatten layer\n",
        "model_1.add(Flatten())\n",
        "# 1nd Dense Layer\n",
        "model_1.add(Dense(128, activation = 'relu'))\n",
        "# 1nd Dropout Layer\n",
        "model_1.add(Dropout(0.5))\n",
        "# 2nd Dense Layer\n",
        "model_1.add(Dense(64, activation = 'relu'))\n",
        "# 2nd Dropout Layer\n",
        "model_1.add(Dropout(0.3))\n",
        "\n",
        "# Final Dense layer => For output prediction 1 mean (binary class in dataset), sigmoid for binary cases\n",
        "model_1.add(Dense(1, activation='sigmoid'))\n",
        "######################### Fully Connected Layer ################################\n",
        "\n",
        "######################### Compile Model ################################\n",
        "model_1.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Summary of the Model Architecture\n",
        "print(model_1.summary())"
      ],
      "metadata": {
        "id": "TezKBegm997_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_normal, count_pneumonia = len(os.listdir(train_normal)), len(os.listdir(train_pneumonia))\n",
        "weight_0 = (1 / count_normal) * (count_normal + count_pneumonia) / 2.0\n",
        "weight_1 = (1 / count_pneumonia) * (count_pneumonia + count_normal) / 2.0\n",
        "\n",
        "class_weights = {0 : weight_0, 1 : weight_1}\n",
        "\n",
        "%time\n",
        "\n",
        "# Fitting / training model\n",
        "history_1 = model_1.fit(train_generator,\n",
        "                        epochs=30,\n",
        "                        batch_size=32,\n",
        "                        validation_data=validation_generator,\n",
        "                        class_weight = class_weights)"
      ],
      "metadata": {
        "id": "KkKMmbkW-KFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history_1.history['accuracy']\n",
        "val_acc = history_1.history['val_accuracy']\n",
        "loss = history_1.history['loss']\n",
        "val_loss = history_1.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r')\n",
        "plt.plot(epochs, val_acc, 'b')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, loss, 'r')\n",
        "plt.plot(epochs, val_loss, 'b')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.title('Training and Validaion Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_ebUeqdSCPO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator.reset()\n",
        "\n",
        "preds_1 = model_1.predict(test_generator,verbose=0)\n",
        "preds_1 = preds_1.copy()\n",
        "preds_1[preds_1 <= 0.5] = 0\n",
        "preds_1[preds_1 > 0.5] = 1\n",
        "\n",
        "# Print Confusion Matrix\n",
        "cm = pd.DataFrame(data=confusion_matrix(test_generator.classes, preds_1, labels=[0, 1]),index=[\"Actual Normal\", \"Actual Pneumonia\"],\n",
        "columns=[\"Predicted Normal\", \"Predicted Pneumonia\"])\n",
        "sns.heatmap(cm,annot=True,fmt=\"d\")\n",
        "\n",
        "# Print Classification Report\n",
        "print(\"\\n\")\n",
        "print(classification_report(y_true=test_generator.classes,y_pred=preds_1,target_names =['Normal','Pneumonia'], digits=4))"
      ],
      "metadata": {
        "id": "vRqOKoxKCe__"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}